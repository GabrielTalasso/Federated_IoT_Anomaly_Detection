{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 10:20:03.333121: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-11-15 10:20:03.333137: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras import layers, losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "#from sklearn.externals import joblib\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.ERROR\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "from model import get_lstm_model, get_conv_model\n",
    "from load_dataset import load_dataset\n",
    "from client_utils import make_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/gabrieltalasso/IoT_Anomaly_Detection/descentralized_model.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gabrieltalasso/IoT_Anomaly_Detection/descentralized_model.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m config \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mserver_round\u001b[39m\u001b[39m'\u001b[39m:\u001b[39mround\u001b[39m}\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gabrieltalasso/IoT_Anomaly_Detection/descentralized_model.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/gabrieltalasso/IoT_Anomaly_Detection/descentralized_model.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     x_train,  x_test \u001b[39m=\u001b[39m load_dataset(dataset_name\u001b[39m=\u001b[39mdataset, cid \u001b[39m=\u001b[39m cid, n_clients \u001b[39m=\u001b[39m\u001b[39m34\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gabrieltalasso/IoT_Anomaly_Detection/descentralized_model.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                                 server_round \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mserver_round\u001b[39m\u001b[39m'\u001b[39m], dataset_size \u001b[39m=\u001b[39m \u001b[39m60\u001b[39m, global_data\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gabrieltalasso/IoT_Anomaly_Detection/descentralized_model.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                                 n_components\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gabrieltalasso/IoT_Anomaly_Detection/descentralized_model.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/gabrieltalasso/IoT_Anomaly_Detection/descentralized_model.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m      \u001b[39mpass\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "loss_type = 'mse'\n",
    "dataset = 'SKAB'\n",
    "model_name = 'CNN'\n",
    "test_name = '1_epoch_descentralized_30c'\n",
    "model_shared = 'All'\n",
    "\n",
    "models = {}\n",
    "\n",
    "n_rounds = 20\n",
    "\n",
    "for round in range(1,n_rounds+1):\n",
    "    for cid in range(30):\n",
    "\n",
    "        config = {'server_round':round}\n",
    "\n",
    "        try:\n",
    "            x_train,  x_test = load_dataset(dataset_name=dataset, cid = cid, n_clients =34,\n",
    "                                        server_round = config['server_round'], dataset_size = 60, global_data= False, \n",
    "                                        n_components=None)\n",
    "        except IndexError:\n",
    "             pass\n",
    "\n",
    "        if config['server_round'] == 1:\n",
    "            models[str(cid)] = get_conv_model(x_train)\n",
    "\n",
    "\n",
    "        model = models[str(cid)]\n",
    "        model.compile(optimizer='adam', loss= loss_type)\n",
    "\n",
    "        loss = pd.Series(np.sum(np.mean(np.abs( x_test -  model.predict( x_test)), axis=1), axis=1)).values[0]\n",
    "\n",
    "        filename = f\"logs/{ dataset}/{ model_name}/{ test_name}/evaluate_before_train/loss_{ loss_type}_{ model_shared}.csv\"\n",
    "        make_logs(filename, config, cid =  cid, loss = loss)\n",
    "\n",
    "\n",
    "        model.compile(optimizer='adam', loss= loss_type)\n",
    "\n",
    "        n_epochs = 1\n",
    "        hist =  model.fit( x_train,  x_train,\n",
    "                epochs = n_epochs, batch_size = 32)\n",
    "            \n",
    "        loss = hist.history['loss'][-1]\t\t\n",
    "        filename = f\"logs/{ dataset}/{ model_name}/{ test_name}/train/loss_{ loss_type}_{ model_shared}.csv\"\n",
    "        make_logs(filename, config, cid =  cid, loss = loss)\n",
    "\n",
    "\n",
    "        models[str(cid)] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
